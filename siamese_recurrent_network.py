import pandas as pd
import numpy as np
import keras
from keras._tf_keras.keras.utils import pad_sequences
from keras._tf_keras.keras.models import Model
from keras._tf_keras.keras import layers

from keras._tf_keras.keras.optimizers import SGD
import tensorflow as tf

MAX_SEQ_LEN = 70


def prepare_dataset(path):
    data = pd.read_csv(path, sep='\t', lineterminator='\n')
    data['is_sim'] = np.where(data['sim'] > 3, 1, 0)
    df = data[['sent_1', 'sent_2', 'is_sim']]
    df = df.dropna(subset=['sent_1', 'sent_2'])
    return df

def prepare_hf_data(data):
    data['is_sim'] = np.where(data['score'] > 3, 1, 0)
    df = data[['sentence1', 'sentence2', 'is_sim']]
    df = df.rename(columns={'sentence1': 'sent_1', 'sentence2': 'sent_2'})
    df = df.dropna(subset=['sent_1', 'sent_2'])
    return df

def prepare_training_data(df, tokenizer):
    sent_1_seq = tokenizer.texts_to_sequences(df['url1'])
    sent_2_seq = tokenizer.texts_to_sequences(df['url2'])

    sent_1_seq = pad_sequences(sent_1_seq, maxlen=MAX_SEQ_LEN)
    sent_2_seq = pad_sequences(sent_2_seq, maxlen=MAX_SEQ_LEN)
    return sent_1_seq, sent_2_seq, df['label'].values


def euclidean_distance(vects):
    x, y = vects
    sum_square = tf.math.reduce_sum(tf.math.square(x - y), axis=1, keepdims=True)
    return tf.math.sqrt(tf.math.maximum(sum_square, tf.keras.backend.epsilon()))


def siameseRN(num_word):
    inp_seq = layers.Input(shape=(MAX_SEQ_LEN,))
    x = layers.Embedding(num_word, output_dim=16, mask_zero=False)(inp_seq) # embed higher dimensional data into lower dimensional vector space
    x = layers.BatchNormalization()(x)
    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)
    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)
    x = layers.GlobalAveragePooling1D()(x)
    x = layers.Dense(128)(x)

    embed_network = keras.Model(inp_seq, x)

    inp_seq1 = layers.Input(shape=(MAX_SEQ_LEN,))
    inp_seq2 = layers.Input(shape=(MAX_SEQ_LEN,))

    network1 = embed_network(inp_seq1)
    network2 = embed_network(inp_seq2)

    merge = layers.Lambda(euclidean_distance)([network1, network2])
    merge = layers.BatchNormalization()(merge)
    out = layers.Dense(1, activation='sigmoid')(merge)

    model = Model(inputs=[inp_seq1, inp_seq2], outputs=out)
    opt = SGD(learning_rate=0.01)
    return model, opt


def loss(margin=1):
    """Provides 'contrastive_loss' an enclosing scope with variable 'margin'.

    Arguments:
        margin: Integer, defines the baseline for distance for which pairs
                should be classified as dissimilar. - (default is 1).

    Returns:
        'contrastive_loss' function with data ('margin') attached.
    """

    # Contrastive loss = mean( (1-true_value) * square(prediction) +
    #                         true_value * square( max(margin-prediction, 0) ))
    def contrastive_loss(y_true, y_pred):
        """Calculates the contrastive loss.

        Arguments:
            y_true: List of labels, each label is of type float32.
            y_pred: List of predictions of same length as of y_true,
                    each label is of type float32.

        Returns:
            A tensor containing contrastive loss as floating point value.
        """
        y_pred = tf.cast(y_pred, tf.float32)
        y_true = tf.cast(y_true, tf.float32)

        square_pred = tf.math.square(y_pred)
        margin_square = tf.math.square(tf.math.maximum(margin - (y_pred), 0))
        return tf.math.reduce_mean(
            (1 - y_true) * square_pred + (y_true) * margin_square
        )

    return contrastive_loss
