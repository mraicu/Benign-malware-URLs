import pandas as pd
import yaml
from sklearn.feature_extraction.text import CountVectorizer
from keras._tf_keras.keras.preprocessing.text import Tokenizer

import wandb
import pickle as pk
from models.ModelFactory import ModelFactory
from siamese_recurrent_network import prepare_training_data, siameseRN, loss
from utils.data_utils import get_data
from utils.file_utils import get_file_name

# Load configuration file
with open('config.yaml', 'r') as file:
    config = yaml.safe_load(file)

# Start a new wandb run to track this script
wandb.init(
    project="B&M-URLs",
    config=config
)

# Get and preprocess data


file_path = get_file_name('', config['model']['name'])

model = None
evals = None
if config['data']['approach'] == 'numerical':

    X_train, y_train = get_data(config['data']['train_path'], config['data']['features'], config['data']['target'])
    X_test, y_test = get_data(config['data']['test_path'], config['data']['features'], config['data']['target'])
    # vectorize data
    vectorizer = CountVectorizer()
    X_train = vectorizer.fit_transform(X_train)
    X_test = vectorizer.transform(X_test)
    file = open('vectorizer.pickle', 'wb')
    pk.dump(vectorizer, file, protocol=pk.HIGHEST_PROTOCOL)
    # Create model
    model = ModelFactory.create_model(config['model']['name'], X_train, y_train, X_test, y_test,
                                      config['model']['parameters'])

    # Train and predict
    model.train()
    y_pred = model.predict()

    file = open(file_path, 'wb')
    pk.dump(model, file, protocol=pk.HIGHEST_PROTOCOL)

    # Calculate evaluation metrics
    evals = model.get_evaluation_metrics()


elif config['data']['approach'] == 'comparative':
    train = pd.read_csv(config['data']['train_path'])
    test = pd.read_csv(config['data']['test_path'])
    tokenizer = Tokenizer(char_level=True)
    tokenizer.fit_on_texts(train['url1'])
    train_text2seq_1, train_text2seq_2, train_label = prepare_training_data(train, tokenizer)
    test_text2seq_1, test_text2seq_2, test_label = prepare_training_data(test, tokenizer)
    # dev_text2seq_1, dev_text2seq_2, dev_label = prepare_training_data(dev, tokenizer)
    # Get model
    num_word = len(tokenizer.word_index) + 1
    model, optimizer = siameseRN(num_word)

    margin = 1
    model.compile(loss=loss(margin=margin), optimizer="adam", metrics=["accuracy"])  # RMSprop
    model.summary()

    model.fit([train_text2seq_1, train_text2seq_2, ], train_label, epochs=config['model']['parameters']['epochs'],
              batch_size=16, verbose=1)

    file = open(file_path, 'wb')
    pk.dump(model, file, protocol=pk.HIGHEST_PROTOCOL)

    # Calculate evaluation metrics
    evals = model.evaluate([test_text2seq_1, test_text2seq_2], test_label, verbose=1)

# Log metrics to Wandb
wandb.log(evals)

# Finish the Wandb run
wandb.finish()
