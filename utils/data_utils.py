import itertools

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer

from utils.FeatureExtraction import FeatureExtractor


def get_data(path, features, target):
    data = pd.read_csv(path)

    X = data[features]
    y = data[target]

    return X.to_numpy(), y.to_numpy()


def get_features(dataframe):
    hostname_size = []
    path_size = []
    count_percent = []
    count_at = []
    count_question_mark = []
    count_hyphen = []
    count_equal = []
    count_full_point = []
    count_http = []
    count_www = []
    count_digits = []
    count_tokens = []
    count_directories = []
    vowel_consonant_ratio_in_hostname = []
    digit_letter_ratio = []
    labels = []

    for url, label in zip(dataframe['url'], dataframe['label']):
        features = FeatureExtractor(url)
        hostname_size.append(features.extract_host_name_size())
        path_size.append(features.extract_path_size())
        count_at.append(features.count_delimiters("@", url))
        count_percent.append(features.count_delimiters("%", url))
        count_question_mark.append(features.count_delimiters("?", url))
        count_hyphen.append(features.count_delimiters("-", url))
        count_equal.append(features.count_delimiters("=", url))
        count_full_point.append(features.count_delimiters(".", url))
        count_http.append(features.count_delimiters("http", url))
        count_www.append(features.count_delimiters("www", url))
        count_digits.append(features.count_digits(url))
        count_tokens.append(features.count_tokens())
        count_directories.append(features.count_directories())
        vowel_consonant_ratio_in_hostname.append(features.vowel_consonant_ratio_in_hostname())
        digit_letter_ratio.append(features.digit_letter_ratio())
        labels.append(label)

    data = {
        'hostNameSize': hostname_size,
        'pathSize': path_size,
        'countPercent': count_percent,
        'countAt': count_at,
        'countQuestionMark': count_question_mark,
        'countHyphen': count_hyphen,
        'countEqual': count_equal,
        'countFullPoint': count_full_point,
        'countHttp': count_http,
        'countWww': count_www,
        'countDigits': count_digits,
        'countTokens': count_tokens,
        'countDirectories': count_directories,
        'vowelConsonantRatioInHostname': vowel_consonant_ratio_in_hostname,
        'digitLetterRatio': digit_letter_ratio,
        'label': labels
    }
    return data


def get_lexical_features(dataframe):
    hostname_size = []
    path_size = []
    count_percent = []
    count_at = []
    count_question_mark = []
    count_hyphen = []
    count_equal = []
    count_full_point = []
    count_http = []
    count_www = []
    count_digits = []
    count_tokens = []
    count_directories = []
    vowel_consonant_ratio_in_hostname = []
    digit_letter_ratio = []

    for url in dataframe['url']:
        features = FeatureExtractor(url)
        hostname_size.append(features.extract_host_name_size())
        path_size.append(features.extract_path_size())
        count_at.append(features.count_delimiters("@", url))
        count_percent.append(features.count_delimiters("%", url))
        count_question_mark.append(features.count_delimiters("?", url))
        count_hyphen.append(features.count_delimiters("-", url))
        count_equal.append(features.count_delimiters("=", url))
        count_full_point.append(features.count_delimiters(".", url))
        count_http.append(features.count_delimiters("http", url))
        count_www.append(features.count_delimiters("www", url))
        count_digits.append(features.count_digits(url))
        count_tokens.append(features.count_tokens())
        count_directories.append(features.count_directories())
        vowel_consonant_ratio_in_hostname.append(features.vowel_consonant_ratio_in_hostname())
        digit_letter_ratio.append(features.digit_letter_ratio())

    data = {
        'hostNameSize': hostname_size,
        'pathSize': path_size,
        'countPercent': count_percent,
        'countAt': count_at,
        'countQuestionMark': count_question_mark,
        'countHyphen': count_hyphen,
        'countEqual': count_equal,
        'countFullPoint': count_full_point,
        'countHttp': count_http,
        'countWww': count_www,
        'countDigits': count_digits,
        'countTokens': count_tokens,
        'countDirectories': count_directories,
        'vowelConsonantRatioInHostname': vowel_consonant_ratio_in_hostname,
        'digitLetterRatio': digit_letter_ratio,
    }
    return data


def get_pairs_features(dataframe):
    # Separate the dataframe by label
    label_groups = dataframe.groupby('label')['url'].apply(list).to_dict()

    # Generate positive pairs (same label)
    positive_pairs = []
    for label, urls in label_groups.items():
        # Generate all possible pairs of URLs within the same label
        positive_pairs.extend(itertools.combinations(urls, 2))

    # Generate negative pairs (different labels)
    labels = list(label_groups.keys())
    negative_pairs = []
    for i in range(len(labels)):
        for j in range(i + 1, len(labels)):
            label_1_urls = label_groups[labels[i]]
            label_2_urls = label_groups[labels[j]]
            negative_pairs.extend(itertools.product(label_1_urls, label_2_urls))

    pairs = [(url1, url2, 1) for url1, url2 in positive_pairs] + \
            [(url1, url2, 0) for url1, url2 in negative_pairs]

    pairs_df = pd.DataFrame(pairs, columns=['url1', 'url2', 'label'])
    shuffled_df = pairs_df.sample(frac=1, random_state=42).reset_index(drop=True)

    return shuffled_df


